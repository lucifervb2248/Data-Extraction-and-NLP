{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b9c497a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\dell\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl requests beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ff241cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped and saved: blackassign0001.txt\n",
      "Successfully scraped and saved: blackassign0002.txt\n",
      "Successfully scraped and saved: blackassign0003.txt\n",
      "Successfully scraped and saved: blackassign0004.txt\n",
      "Successfully scraped and saved: blackassign0005.txt\n",
      "Successfully scraped and saved: blackassign0006.txt\n",
      "Successfully scraped and saved: blackassign0007.txt\n",
      "Successfully scraped and saved: blackassign0008.txt\n",
      "Successfully scraped and saved: blackassign0009.txt\n",
      "Successfully scraped and saved: blackassign0010.txt\n",
      "Successfully scraped and saved: blackassign0011.txt\n",
      "Successfully scraped and saved: blackassign0012.txt\n",
      "Successfully scraped and saved: blackassign0013.txt\n",
      "Successfully scraped and saved: blackassign0014.txt\n",
      "Successfully scraped and saved: blackassign0015.txt\n",
      "Successfully scraped and saved: blackassign0016.txt\n",
      "Successfully scraped and saved: blackassign0017.txt\n",
      "Successfully scraped and saved: blackassign0018.txt\n",
      "Successfully scraped and saved: blackassign0019.txt\n",
      "Successfully scraped and saved: blackassign0020.txt\n",
      "Successfully scraped and saved: blackassign0021.txt\n",
      "Successfully scraped and saved: blackassign0022.txt\n",
      "Successfully scraped and saved: blackassign0023.txt\n",
      "Successfully scraped and saved: blackassign0024.txt\n",
      "Successfully scraped and saved: blackassign0025.txt\n",
      "Successfully scraped and saved: blackassign0026.txt\n",
      "Successfully scraped and saved: blackassign0027.txt\n",
      "Successfully scraped and saved: blackassign0028.txt\n",
      "Successfully scraped and saved: blackassign0029.txt\n",
      "Successfully scraped and saved: blackassign0030.txt\n",
      "Successfully scraped and saved: blackassign0031.txt\n",
      "Successfully scraped and saved: blackassign0032.txt\n",
      "Successfully scraped and saved: blackassign0033.txt\n",
      "Successfully scraped and saved: blackassign0034.txt\n",
      "Successfully scraped and saved: blackassign0035.txt\n",
      "Failed to retrieve https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Successfully scraped and saved: blackassign0037.txt\n",
      "Successfully scraped and saved: blackassign0038.txt\n",
      "Successfully scraped and saved: blackassign0039.txt\n",
      "Successfully scraped and saved: blackassign0040.txt\n",
      "Successfully scraped and saved: blackassign0041.txt\n",
      "Successfully scraped and saved: blackassign0042.txt\n",
      "Successfully scraped and saved: blackassign0043.txt\n",
      "Successfully scraped and saved: blackassign0044.txt\n",
      "Successfully scraped and saved: blackassign0045.txt\n",
      "Successfully scraped and saved: blackassign0046.txt\n",
      "Successfully scraped and saved: blackassign0047.txt\n",
      "Successfully scraped and saved: blackassign0048.txt\n",
      "Failed to retrieve https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Successfully scraped and saved: blackassign0050.txt\n",
      "Successfully scraped and saved: blackassign0051.txt\n",
      "Successfully scraped and saved: blackassign0052.txt\n",
      "Successfully scraped and saved: blackassign0053.txt\n",
      "Successfully scraped and saved: blackassign0054.txt\n",
      "Successfully scraped and saved: blackassign0055.txt\n",
      "Successfully scraped and saved: blackassign0056.txt\n",
      "Successfully scraped and saved: blackassign0057.txt\n",
      "Successfully scraped and saved: blackassign0058.txt\n",
      "Successfully scraped and saved: blackassign0059.txt\n",
      "Successfully scraped and saved: blackassign0060.txt\n",
      "Successfully scraped and saved: blackassign0061.txt\n",
      "Successfully scraped and saved: blackassign0062.txt\n",
      "Successfully scraped and saved: blackassign0063.txt\n",
      "Successfully scraped and saved: blackassign0064.txt\n",
      "Successfully scraped and saved: blackassign0065.txt\n",
      "Successfully scraped and saved: blackassign0066.txt\n",
      "Successfully scraped and saved: blackassign0067.txt\n",
      "Successfully scraped and saved: blackassign0068.txt\n",
      "Successfully scraped and saved: blackassign0069.txt\n",
      "Successfully scraped and saved: blackassign0070.txt\n",
      "Successfully scraped and saved: blackassign0071.txt\n",
      "Successfully scraped and saved: blackassign0072.txt\n",
      "Successfully scraped and saved: blackassign0073.txt\n",
      "Successfully scraped and saved: blackassign0074.txt\n",
      "Successfully scraped and saved: blackassign0075.txt\n",
      "Successfully scraped and saved: blackassign0076.txt\n",
      "Successfully scraped and saved: blackassign0077.txt\n",
      "Successfully scraped and saved: blackassign0078.txt\n",
      "Successfully scraped and saved: blackassign0079.txt\n",
      "Successfully scraped and saved: blackassign0080.txt\n",
      "Successfully scraped and saved: blackassign0081.txt\n",
      "Successfully scraped and saved: blackassign0082.txt\n",
      "Successfully scraped and saved: blackassign0083.txt\n",
      "Successfully scraped and saved: blackassign0084.txt\n",
      "Successfully scraped and saved: blackassign0085.txt\n",
      "Successfully scraped and saved: blackassign0086.txt\n",
      "Successfully scraped and saved: blackassign0087.txt\n",
      "Successfully scraped and saved: blackassign0088.txt\n",
      "Successfully scraped and saved: blackassign0089.txt\n",
      "Successfully scraped and saved: blackassign0090.txt\n",
      "Successfully scraped and saved: blackassign0091.txt\n",
      "Successfully scraped and saved: blackassign0092.txt\n",
      "Successfully scraped and saved: blackassign0093.txt\n",
      "Successfully scraped and saved: blackassign0094.txt\n",
      "Successfully scraped and saved: blackassign0095.txt\n",
      "Successfully scraped and saved: blackassign0096.txt\n",
      "Successfully scraped and saved: blackassign0097.txt\n",
      "Successfully scraped and saved: blackassign0098.txt\n",
      "Successfully scraped and saved: blackassign0099.txt\n",
      "Successfully scraped and saved: blackassign0100.txt\n",
      "Scraping done.\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "xlsx_file_path = 'Input.xlsx'\n",
    "output_folder = './scraped_texts1/'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "wb = openpyxl.load_workbook(xlsx_file_path)\n",
    "ws = wb.active\n",
    "\n",
    "for row in ws.iter_rows(min_row=2, values_only=True):\n",
    "    filename, url = row[0], row[1]\n",
    "    output_file_path = os.path.join(output_folder, f\"{filename}.txt\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            \n",
    "            title_tag = soup.find('title')\n",
    "            title = title_tag.get_text(strip=True) if title_tag else 'Title Not Found'\n",
    "        \n",
    "        \n",
    "            content = []\n",
    "            content.append(title)\n",
    "            dc=['td-post-content tagdiv-type','tdb-block-inner td-fix-index']\n",
    "            p_strong = soup.find('div', class_='td-post-content tagdiv-type' )\n",
    "            p_strong2= soup.find_all('div', class_='tdb-block-inner td-fix-index' )\n",
    "            if(p_strong):\n",
    "                text = p_strong.get_text(strip=True, separator='\\n')\n",
    "                content.append(text)\n",
    "            elif(p_strong2):\n",
    "                for p in p_strong2:\n",
    "                \n",
    "                    text2 = p.get_text(strip=True, separator='\\n')\n",
    "\n",
    "                    content.append(text2)\n",
    "           \n",
    "            text = '\\n'.join(content)\n",
    "            \n",
    "            \n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                output_file.write(text)\n",
    "            print(f'Successfully scraped and saved: {filename}.txt')\n",
    "        else:\n",
    "            print(f'Failed to retrieve {url}')\n",
    "    except Exception as e:\n",
    "        print(f'Error scraping {url}: {e}')\n",
    "\n",
    "print('Scraping done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "10cf9567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined stopwords written to combined_stopwords.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to the folder containing text files\n",
    "folder_path = \"./StopWords/\"\n",
    "\n",
    "# List to store paths of text files\n",
    "text_files = []\n",
    "\n",
    "# Iterate over all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if file_name.endswith('.txt') and os.path.isfile(file_path):\n",
    "        text_files.append(file_path)\n",
    "\n",
    "# Now text_files list contains paths to all text files in the folder\n",
    "unique_stopwords = []\n",
    "\n",
    "# Iterate over each stopwords file\n",
    "for file_name in text_files:\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            word = line.strip()  \n",
    "            if word:  \n",
    "                unique_stopwords.append(word)\n",
    "\n",
    "# Output file to store combined stopwords\n",
    "output_file = 'combined_stopwords.txt'\n",
    "\n",
    "# Write unique stopwords to the output file\n",
    "with open(output_file, 'w') as file:\n",
    "    for word in (unique_stopwords):\n",
    "        file.write(word + '\\n')\n",
    "\n",
    "print(f\"Combined stopwords written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2edcba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words after '|' removed. Output written to output.txt\n"
     ]
    }
   ],
   "source": [
    "# Input file path\n",
    "input_file_path = \"combined_stopwords.txt\"\n",
    "\n",
    "# Output file path\n",
    "output_file_path = \"output.txt\"\n",
    "\n",
    "# Read input file and process each line\n",
    "with open(input_file_path, 'r') as input_file:\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        for line in input_file:\n",
    "            \n",
    "            parts = line.split('|')\n",
    "            modified_line = parts[0]\n",
    "            output_file.write(modified_line)\n",
    "\n",
    "print(\"Words after '|' removed. Output written to\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92154c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords removed from text files. Output written to ./swscraped_texts1/\n",
      "Stopwords written to stopwords_removed.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "# Function to remove stopwords from text\n",
    "def remove_stopwords(text, stopwords):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# File paths\n",
    "stopwords_file = \"output.txt\"\n",
    "input_folder = \"./scraped_texts1/\"\n",
    "output_folder = \"./swscraped_texts1/\"\n",
    "stopwords_output_file = \"stopwords_removed.txt\"\n",
    "\n",
    "# Read stopwords from file and store in a set\n",
    "stopwords_set = set()\n",
    "with open(stopwords_file, 'r') as f:\n",
    "    for line in f:\n",
    "        stopwords_set.update(line.split())\n",
    "\n",
    "# Process each text file\n",
    "for filename in os.listdir(input_folder):\n",
    "    input_file_path = os.path.join(input_folder, filename)\n",
    "    output_file_path = os.path.join(output_folder, filename)\n",
    "    \n",
    "    with open(input_file_path, 'r') as input_file:\n",
    "        text = input_file.read()\n",
    "        text=''.join(char for char in text if char not in string.punctuation)\n",
    "        processed_text = remove_stopwords(text, stopwords_set)\n",
    "        \n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            output_file.write(processed_text)\n",
    "\n",
    "# Write stopwords to separate file\n",
    "with open(stopwords_output_file, 'w') as f:\n",
    "    f.write(' '.join(stopwords_set))\n",
    "\n",
    "print(\"Stopwords removed from text files. Output written to\", output_folder)\n",
    "print(\"Stopwords written to\", stopwords_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08714fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('cmudict')\n",
    "\n",
    "\n",
    "cmu_dict = cmudict.dict()\n",
    "def count_personal_pronouns(text):\n",
    "    pronoun_pattern = r'\\b(?:I|we|my|ours|us)\\b'\n",
    "    regex = re.compile(pronoun_pattern, re.IGNORECASE)\n",
    "    matches = regex.findall(text)\n",
    "    filtered_matches = [match for match in matches if match.lower() != \"us\"]\n",
    "    pronoun_count = len(filtered_matches)\n",
    "    return pronoun_count\n",
    "\n",
    "def count_syllables(word):\n",
    "    \n",
    "    if word.endswith(('es', 'ed')):\n",
    "        return 0\n",
    "\n",
    "    vowels = re.findall(r'[aeiouyAEIOUY]+', word)\n",
    "    return len(vowels)\n",
    "def count_syllables_in_text(text):\n",
    "    words = text.split()\n",
    "    syllable_counts = [count_syllables(word) for word in words]\n",
    "    return syllable_counts\n",
    "\n",
    "# Function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    phonemes = cmu_dict.get(word.lower())\n",
    "    if phonemes:\n",
    "        return len([ph for ph in phonemes[0] if ph[-1].isdigit()])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text_without_punctuation = text.translate(translator)\n",
    "    return text_without_punctuation\n",
    "\n",
    "# Function to count complex words in a text\n",
    "def count_complex_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    complex_words = [word for word in words if count_syllables(word) > 2]\n",
    "    return len(complex_words)\n",
    "\n",
    "# Function to count positive and negative words in a text\n",
    "def count_positive_negative(text, positive_words, negative_words):\n",
    "    positive_count = sum(1 for word in text.split() if word.lower() in positive_words)\n",
    "    negative_count = sum(1 for word in text.split() if word.lower() in negative_words)\n",
    "    return positive_count, negative_count\n",
    "def count_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return len(sentences)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read positive and negative words from text files\n",
    "with open('positive-words.txt', 'r') as positive_file:\n",
    "    positive_words = set(positive_file.read().split())\n",
    "\n",
    "with open('negative-words.txt', 'r') as negative_file:\n",
    "    negative_words = set(negative_file.read().split())\n",
    "\n",
    "# Folder containing text files\n",
    "folder_path = './swscraped_texts1/'\n",
    "\n",
    "# Create and open the CSV file for writing\n",
    "with open('Output Data Structure.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['Filename', 'Positive score', 'Negative score','Polarity score','Subjectivity Score','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Iterate over each file in the folder\n",
    "   \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the content of the file\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                text_wp = remove_punctuation(text)\n",
    "                words = text_wp.split()\n",
    "    \n",
    "                word_count = len(words)\n",
    "                positive_count, negative_count = count_positive_negative(text, positive_words, negative_words)\n",
    "                positive_count=int(positive_count)\n",
    "                negative_count=int(negative_count)\n",
    "                polarity_score=(positive_count-negative_count)/ ((positive_count + negative_count) + 0.000001)\n",
    "                sub_score=(positive_count+negative_count)/ ((word_count) + 0.000001)\n",
    "                complex_word_count = count_complex_words(text)\n",
    "    \n",
    "                sentence_count = count_sentences(text)\n",
    "                avg_s_len=word_count/sentence_count\n",
    "                pt_cw=complex_word_count /word_count\n",
    "                fog_index=0.4*(avg_s_len+pt_cw)\n",
    "                avg_num_wps=word_count/sentence_count\n",
    "                syllable_counts = count_syllables_in_text(text)\n",
    "                scpw=sum(syllable_counts)/word_count\n",
    "                pronoun_count = count_personal_pronouns(text)\n",
    "                total_length = sum(len(word) for word in words)\n",
    "                avg_word_len=total_length/word_count\n",
    "            \n",
    "\n",
    "            # Write the counts to the CSV file\n",
    "            writer.writerow({'Filename': filename, 'Positive score': positive_count, 'Negative score': negative_count,'Polarity score':polarity_score,'Subjectivity Score':sub_score,'AVG SENTENCE LENGTH':avg_s_len,'PERCENTAGE OF COMPLEX WORDS':pt_cw,'FOG INDEX':fog_index,'AVG NUMBER OF WORDS PER SENTENCE':avg_num_wps,'COMPLEX WORD COUNT':complex_word_count,'WORD COUNT':word_count,'SYLLABLE PER WORD':scpw,'PERSONAL PRONOUNS':pronoun_count,'AVG WORD LENGTH':avg_word_len,})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3bdb5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003c641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
